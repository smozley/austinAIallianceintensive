{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smozley/austinAIallianceintensive/blob/main/multi_agent_orchestration_tutorial_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This tutorial guides you through a simple multi-agent network system from scratch.\n",
        "\n",
        "### **Step 1: Setup and Installation**"
      ],
      "metadata": {
        "id": "jv1ozXDS1UgI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up Ollama (in the terminal window)"
      ],
      "metadata": {
        "id": "mwodPRxN1Nzw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Install the dependencies\n",
        "\n",
        "```\n",
        "sudo apt update\n",
        "sudo apt install -y pciutils\n",
        "curl -fsSL https://ollama.com/install.sh | sh\n",
        "```\n",
        "\n",
        "2. Run the ollama server in the background\n",
        "\n",
        "```\n",
        "ollama serve &\n",
        "```\n",
        "\n",
        "3. Pull the required models\n",
        "\n",
        "```\n",
        "ollama pull granite3.3:latest\n",
        "```\n",
        "\n",
        "4. Run the models in the background\n",
        "\n",
        "```\n",
        "ollama run granite3.3:latest &\n",
        "```"
      ],
      "metadata": {
        "id": "WYvOV7K_1L6D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing the necessary libraries"
      ],
      "metadata": {
        "id": "xr9c-In81dp7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1pDdlOx90YM3"
      },
      "outputs": [],
      "source": [
        "!pip install langgraph langchain langchain_openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 2: Initialize Model**\n",
        "\n",
        "We will configure our LLMs using the `ChatOpenAI` wrapper."
      ],
      "metadata": {
        "id": "2tKeY2EX1mEQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import TypedDict, Annotated, Literal\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.messages import BaseMessage, HumanMessage\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langgraph.graph import StateGraph, END\n",
        "from langgraph.graph.message import add_messages\n",
        "\n",
        "# This connects to your local Ollama instance.\n",
        "# Make sure the model name matches what you have pulled.\n",
        "# You can use \"granite3.3:latest\", \"mistral:latest\", etc.\n",
        "llm = ChatOpenAI(\n",
        "    base_url='http://localhost:11434/v1',\n",
        "    api_key='ollama',  # required but can be any string\n",
        "    model='granite3.3:latest',\n",
        "    temperature=0.9, # A little creativity for joke writing\n",
        ")\n",
        "\n",
        "print(\"✅ LLM Initialized\")"
      ],
      "metadata": {
        "id": "F5T6IjDY0ZCq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 3: Define Agent State and Nodes**"
      ],
      "metadata": {
        "id": "86JO0yKI_UQN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This is the shared \"story document\" for our agents.\n",
        "class StoryState(TypedDict):\n",
        "    # The list of messages that make up the story so far.\n",
        "    messages: Annotated[list[BaseMessage], add_messages]\n",
        "    # A turn counter to control the length of the story.\n",
        "    turn: int\n",
        "    # The maximum number of turns for the story.\n",
        "    max_turns: int\n",
        "\n",
        "\n",
        "# These are our two specialist writers.\n",
        "\n",
        "def world_builder_node(state: StoryState):\n",
        "    \"\"\"\n",
        "    The World Builder agent. It focuses on creating the setting and atmosphere.\n",
        "    \"\"\"\n",
        "    print(f\" Turn {state['turn'] + 1}: World Builder is writing... \")\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\",\n",
        "         \"You are a master world-builder. Your role is to describe the setting, characters, and atmosphere. \"\n",
        "         \"Read the story so far, and add a paragraph that expands on the description of the world. \"\n",
        "         \"Do NOT advance the plot or make characters speak. Focus only on descriptive details.\"),\n",
        "        (\"placeholder\", \"{messages}\")\n",
        "    ])\n",
        "\n",
        "    chain = prompt | llm | StrOutputParser()\n",
        "    story_segment = chain.invoke({\"messages\": state['messages']})\n",
        "\n",
        "    print(story_segment)\n",
        "    return {\n",
        "        \"messages\": [HumanMessage(content=story_segment, name=\"World Builder\")],\n",
        "        \"turn\": state['turn'] + 1\n",
        "    }\n",
        "\n",
        "\n",
        "def plot_weaver_node(state: StoryState):\n",
        "    \"\"\"\n",
        "    The Plot Weaver agent. It focuses on action, dialogue, and advancing the story.\n",
        "    \"\"\"\n",
        "    print(f\" Turn {state['turn'] + 1}: Plot Weaver is writing... \")\n",
        "\n",
        "    prompt = ChatPromptTemplate.from_messages([\n",
        "        (\"system\",\n",
        "         \"You are a master storyteller. Your role is to advance the plot. \"\n",
        "         \"Read the story so far, including the world-building, and add a paragraph that introduces an action, a decision, or a line of dialogue. \"\n",
        "         \"Do NOT add descriptive details about the setting. Focus only on moving the story forward.\"),\n",
        "        (\"placeholder\", \"{messages}\")\n",
        "    ])\n",
        "\n",
        "    chain = prompt | llm | StrOutputParser()\n",
        "    story_segment = chain.invoke({\"messages\": state['messages']})\n",
        "\n",
        "    print(story_segment)\n",
        "    return {\n",
        "        \"messages\": [HumanMessage(content=story_segment, name=\"Plot Weaver\")],\n",
        "        \"turn\": state['turn'] + 1\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "def router_node(state: StoryState) -> Literal[\"world_builder\", \"plot_weaver\", END]:\n",
        "    \"\"\"\n",
        "    This function determines which agent should write next.\n",
        "    \"\"\"\n",
        "    # If we have reached the maximum number of turns, the story ends.\n",
        "    if state['turn'] >= state['max_turns']:\n",
        "        print(\" Story Complete \")\n",
        "        return END\n",
        "\n",
        "    # Check the name of the last speaker to decide who goes next.\n",
        "    last_speaker = state[\"messages\"][-1].name\n",
        "    if last_speaker == \"World Builder\":\n",
        "        return \"plot_weaver\"\n",
        "    else:\n",
        "        return \"world_builder\""
      ],
      "metadata": {
        "id": "nk7DAr920vE2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 4: Build the Multi-Agent Network Graph**\n",
        "\n",
        "Now we wire the nodes together."
      ],
      "metadata": {
        "id": "xYbt1HCD_fhH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This graph implements the dynamic, peer-to-peer collaboration.\n",
        "\n",
        "workflow = StateGraph(StoryState)\n",
        "\n",
        "# Add the two specialist agent nodes.\n",
        "workflow.add_node(\"world_builder\", world_builder_node)\n",
        "workflow.add_node(\"plot_weaver\", plot_weaver_node)\n",
        "\n",
        "# The story always starts with the World Builder to set the scene.\n",
        "workflow.set_entry_point(\"world_builder\")\n",
        "\n",
        "# Define the conditional routing.\n",
        "# After the World Builder, we call the router to decide the next step.\n",
        "workflow.add_conditional_edges(\n",
        "    \"world_builder\",\n",
        "    router_node,\n",
        "    {\"plot_weaver\": \"plot_weaver\", END: END}\n",
        ")\n",
        "# After the Plot Weaver, we also call the router.\n",
        "workflow.add_conditional_edges(\n",
        "    \"plot_weaver\",\n",
        "    router_node,\n",
        "    {\"world_builder\": \"world_builder\", END: END}\n",
        ")\n",
        "\n",
        "# Compile the graph.\n",
        "graph = workflow.compile()\n",
        "print(\"✅ Creative Writing Duo Graph Compiled\")"
      ],
      "metadata": {
        "id": "_XcIl4-x02MK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
        "except Exception:\n",
        "    pass"
      ],
      "metadata": {
        "id": "KaKSRXJy1BW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 5: Run the Graph**"
      ],
      "metadata": {
        "id": "0M6uBSCU_nTp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    story_premise = \"A lone astronaut discovers a mysterious, silent alien monolith on Mars.\"\n",
        "    max_turns = 6 # The story will have 6 paragraphs (3 from each agent).\n",
        "\n",
        "    print(f\"\\n Beginning a new story: '{story_premise}'\\n\")\n",
        "\n",
        "    # Using .invoke() is simpler for getting the final result after the full run.\n",
        "    # It waits for the graph to complete and returns the final, merged state.\n",
        "    final_state = graph.invoke(\n",
        "        {\n",
        "            \"messages\": [HumanMessage(content=story_premise, name=\"Story Premise\")],\n",
        "            \"turn\": 0,\n",
        "            \"max_turns\": max_turns\n",
        "        },\n",
        "        # Set a high recursion limit to allow for many turns in the conversation.\n",
        "        {\"recursion_limit\": 100}\n",
        "    )\n",
        "\n",
        "    print(\"\\n\\n\" + \"#\"*60)\n",
        "    print(\"           THE COMPLETE STORY\")\n",
        "    print(\"#\"*60 + \"\\n\")\n",
        "\n",
        "    full_story = \"\\n\\n\".join([msg.content for msg in final_state['messages']])\n",
        "    print(full_story)"
      ],
      "metadata": {
        "id": "1rTrLNxG0_Yt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}