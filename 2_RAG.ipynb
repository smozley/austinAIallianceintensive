{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "rTI2Vfzkc-Mz",
        "O1OEKvpkeoSh",
        "6kmu2QL8fZzm",
        "hMoxB_dRgDfr",
        "qsJMbxbWhIL9"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smozley/austinAIallianceintensive/blob/main/2_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic RAG Demo"
      ],
      "metadata": {
        "id": "azQUUv1Qc6EI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Extraction"
      ],
      "metadata": {
        "id": "rTI2Vfzkc-Mz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF"
      ],
      "metadata": {
        "id": "BHwumUF4dsnz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOhFCkxfc2r7"
      },
      "outputs": [],
      "source": [
        "import fitz  # PyMuPDF\n",
        "\n",
        "# Load the PDF\n",
        "pdf_path = \"/content/sample_data/tesla_manual.pdf\"\n",
        "doc = fitz.open(pdf_path)\n",
        "\n",
        "# Extract text from all pages\n",
        "pages = [page.get_text() for page in doc]\n",
        "full_text = \"\\n\".join(pages)\n",
        "\n",
        "print(f\"Extracted {len(pages)} pages of text.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(full_text[:1000])"
      ],
      "metadata": {
        "id": "EHUueixRderR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Chunking"
      ],
      "metadata": {
        "id": "O1OEKvpkeoSh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "id": "OtM-5j4Wem7C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "sample = full_text[10000:20000]\n",
        "\n",
        "# For this we will use a RecursiveCharacterTextSplitter from langchain\n",
        "custom_text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1024,\n",
        "    chunk_overlap=20,\n",
        "    length_function=len\n",
        ")\n",
        "\n",
        "# Create the chunks\n",
        "texts = custom_text_splitter.create_documents([sample])\n",
        "\n",
        "# Convert to a list of plain text chunks for further processing\n",
        "chunks = [doc.page_content for doc in texts]\n",
        "\n",
        "print(f\"Total Chunks Created: {len(chunks)}\\n\\n\")\n",
        "# Print the first two chunks\n",
        "print(f'### Chunk 1:\\n\\n{chunks[10]}\\n\\n=====\\n')\n",
        "print(f'### Chunk 2:\\n\\n{chunks[11]}\\n\\n=====')"
      ],
      "metadata": {
        "id": "qK3qKEuZetvV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Embedding + BM25"
      ],
      "metadata": {
        "id": "6kmu2QL8fZzm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load embedding model\n",
        "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
        "\n",
        "# Compute dense embeddings\n",
        "embeddings = model.encode(chunks, convert_to_numpy=True)\n",
        "\n",
        "# Compute sparse representations (TF-IDF as BM25 approximation)\n",
        "tfidf = TfidfVectorizer().fit(chunks)\n",
        "bm25_matrix = tfidf.transform(chunks)\n",
        "\n",
        "# Store both in a simple DataFrame (this will take a bit)\n",
        "index = pd.DataFrame({\n",
        "    \"chunk\": chunks,\n",
        "    \"embedding\": list(embeddings),\n",
        "    \"tfidf\": list(bm25_matrix)\n",
        "})\n",
        "\n",
        "index.tail(2)"
      ],
      "metadata": {
        "id": "rFWCh1I4fgx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Retrieval"
      ],
      "metadata": {
        "id": "hMoxB_dRgDfr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from scipy.sparse import vstack\n",
        "\n",
        "def hybrid_search(query, top_k=5, alpha=0.5):\n",
        "    # Get embedding and sparse vector for query\n",
        "    query_embedding = model.encode([query], convert_to_numpy=True)\n",
        "    query_tfidf = tfidf.transform([query])\n",
        "\n",
        "    # Dense similarity (embeddings)\n",
        "    dense_matrix = np.stack(index[\"embedding\"].values)\n",
        "    dense_scores = cosine_similarity(query_embedding, dense_matrix)[0]\n",
        "\n",
        "    # Sparse similarity (TF-IDF)\n",
        "    tfidf_matrix = vstack(index[\"tfidf\"].values)\n",
        "    sparse_scores = cosine_similarity(query_tfidf, tfidf_matrix)[0]\n",
        "\n",
        "    # Combine scores\n",
        "    hybrid_scores = alpha * dense_scores + (1 - alpha) * sparse_scores\n",
        "\n",
        "    # Rank and return top_k chunks\n",
        "    top_indices = np.argsort(hybrid_scores)[::-1][:top_k]\n",
        "    return index.iloc[top_indices].copy()\n",
        "\n",
        "# Example query\n",
        "hits = hybrid_search(\"How do I charge the Tesla vehicle?\", top_k=5)\n",
        "hits[[\"chunk\"]]"
      ],
      "metadata": {
        "id": "Te0-mOD-gBBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Generation"
      ],
      "metadata": {
        "id": "qsJMbxbWhIL9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "VCTZykq6hLUR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install anthropic"
      ],
      "metadata": {
        "id": "jtHDT-ephjGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import anthropic\n",
        "from google.colab import userdata\n",
        "from pydantic import BaseModel\n",
        "from enum import StrEnum\n",
        "\n",
        "client = anthropic.Anthropic(api_key=userdata.get('ANTHROPIC_API_KEY'))\n",
        "\n",
        "class Model(StrEnum):\n",
        "  SM = 'claude-3-5-haiku-20241022'\n",
        "  MD = 'claude-sonnet-4-20250514'\n",
        "  LG = 'claude-opus-4-20250514'\n",
        "\n",
        "class Question(BaseModel):\n",
        "  model: Model\n",
        "  prompt: str\n",
        "\n",
        "from typing import Generator\n",
        "\n",
        "def stream_claude(question: Question) -> Generator[str, None, None]:\n",
        "  assert question.model in [Model.MD, Model.LG]\n",
        "\n",
        "  with client.messages.stream(\n",
        "        model=question.model.value,\n",
        "        max_tokens=1024,\n",
        "        messages=[{\"role\": \"user\", \"content\": question.prompt}]\n",
        "    ) as stream:\n",
        "        for text in stream.text_stream:\n",
        "            yield text"
      ],
      "metadata": {
        "id": "peeFwVU9g9Q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ask Questions"
      ],
      "metadata": {
        "id": "ymPgdLWMiuPO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"How do I open the front doors manually?\"\n",
        "hits = hybrid_search(query, top_k=5)"
      ],
      "metadata": {
        "id": "_X_Pi2yvixYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine retrieved chunks\n",
        "context = \"\\n\".join(hits[\"chunk\"].values)\n",
        "\n",
        "# Prompt template\n",
        "prompt = f\"\"\"Use the context below to answer the question.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {query}\n",
        "Answer:\"\"\"\n",
        "\n",
        "# Create the question and take a look\n",
        "question = Question(model=Model.MD, prompt=prompt)\n",
        "print(question.prompt)"
      ],
      "metadata": {
        "id": "IpqlZ_q9gHSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate response\n",
        "for chunk in stream_claude(question):\n",
        "    print(chunk, end=\"\", flush=True)"
      ],
      "metadata": {
        "id": "Jj4YeOjiiEQ6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}