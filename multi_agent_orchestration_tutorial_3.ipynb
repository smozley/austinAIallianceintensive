{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smozley/austinAIallianceintensive/blob/main/multi_agent_orchestration_tutorial_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This tutorial will guide you through building a hierarchical, multi-agent system.\n",
        "\n",
        "We will create a **Supervisor** agent (powered by Granite) that orchestrates two distinct sub-teams: a **Research Team** and a **Document Authoring Team**. This demonstrates a powerful, nested hierarchical structure where the supervisor delegates high-level tasks to sub-teams, who then manage their own set of specialized worker agents.\n",
        "\n",
        "The entire system uses LangGraph for orchestration, local models via Ollama, a custom MCP for tool calling, and Langfuse for complete observability.\n",
        "\n",
        "### **Step 1: Setup and Installation**\n",
        "\n",
        "First, ensure Ollama is running and that you have the required models."
      ],
      "metadata": {
        "id": "VXA24xwFTfLs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting up Ollama (in the terminal window)"
      ],
      "metadata": {
        "id": "5v9XFH2LU7K2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Install the dependencies\n",
        "\n",
        "```\n",
        "sudo apt update\n",
        "sudo apt install -y pciutils\n",
        "curl -fsSL https://ollama.com/install.sh | sh\n",
        "```\n",
        "\n",
        "2. Run the ollama server in the background\n",
        "\n",
        "```\n",
        "ollama serve &\n",
        "```\n",
        "\n",
        "3. Pull the required models\n",
        "\n",
        "```\n",
        "ollama pull granite3.3:latest\n",
        "ollama pull mistral:latest\n",
        "```\n",
        "\n",
        "4. Run the models in the background\n",
        "\n",
        "```\n",
        "ollama run granite3.3:latest &\n",
        "ollama run mistral:latest &\n",
        "```"
      ],
      "metadata": {
        "id": "1F7eJW10U2T9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Installing the necessary libraries"
      ],
      "metadata": {
        "id": "3sU4Cs-6U_dm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install all required packages\n",
        "!pip install -qU langgraph langchain_openai langchain_community langchain-tavily langfuse requests beautifulsoup4 altair vl-convert-python"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "fnUCrVgPTfLw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **Step 2: Configure Langfuse & API Keys**\n",
        "\n",
        "Next, we'll set up credentials for **Langfuse** (for observability) and **Tavily** (for the web search tool)."
      ],
      "metadata": {
        "id": "sss38MV8TfLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from getpass import getpass\n",
        "from langfuse import Langfuse\n",
        "\n",
        "#  Langfuse Configuration for Observability\n",
        "# Get keys from your Langfuse project at https://cloud.langfuse.com\n",
        "if \"LANGFUSE_PUBLIC_KEY\" not in os.environ:\n",
        "    os.environ[\"LANGFUSE_PUBLIC_KEY\"] = getpass(\"Enter your Langfuse Public Key: \")\n",
        "if \"LANGFUSE_SECRET_KEY\" not in os.environ:\n",
        "    os.environ[\"LANGFUSE_SECRET_KEY\"] = getpass(\"Enter your Langfuse Secret Key: \")\n",
        "if \"LANGFUSE_HOST\" not in os.environ:\n",
        "    os.environ[\"LANGFUSE_HOST\"] = \"https://us.cloud.langfuse.com\"\n",
        "\n",
        "# Initialize the Langfuse handler to trace our runs\n",
        "langfuse_handler = Langfuse()\n",
        "\n",
        "#  Tavily API Key for the search tool\n",
        "if \"TAVILY_API_KEY\" not in os.environ:\n",
        "    os.environ[\"TAVILY_API_KEY\"] = getpass(\"Enter your Tavily API key: \")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "jfhvwhxcTfLx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **Step 3: Initialize Models and Tools**\n",
        "\n",
        "We will configure our LLMs using the `ChatOpenAI` wrapper, which enables advanced features like tool calling with local Ollama models."
      ],
      "metadata": {
        "id": "Byp7qOO3TfLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import json\n",
        "import vl_convert as vlc\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_tavily import TavilySearch\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "#  Common Ollama Configuration\n",
        "OLLAMA_BASE_URL = \"http://localhost:11434/v1\"\n",
        "DUMMY_API_KEY = \"ollama\"\n",
        "\n",
        "supervisor_llm = ChatOpenAI(\n",
        "    model=\"granite3.3:latest\",\n",
        "    api_key=DUMMY_API_KEY,\n",
        "    base_url=OLLAMA_BASE_URL,\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "granite_worker_llm = ChatOpenAI(\n",
        "    model=\"granite3.3:latest\",\n",
        "    api_key=DUMMY_API_KEY,\n",
        "    base_url=OLLAMA_BASE_URL,\n",
        "    temperature=0\n",
        ")\n",
        "\n",
        "mistral_worker_llm = ChatOpenAI(\n",
        "    model=\"mistral:latest\",\n",
        "    api_key=DUMMY_API_KEY,\n",
        "    base_url=OLLAMA_BASE_URL,\n",
        "    temperature=0.3\n",
        ")\n",
        "\n",
        "#  Tool Definitions\n",
        "@tool\n",
        "def web_search(query: str) -> str:\n",
        "    \"\"\"Performs a web search using Tavily and returns the results.\"\"\"\n",
        "    tavily_tool = TavilySearch(max_results=3)\n",
        "    return tavily_tool.invoke(query)\n",
        "\n",
        "@tool\n",
        "def web_scraper(url: str) -> str:\n",
        "    \"\"\"Fetches and extracts clean text content from a given URL.\"\"\"\n",
        "    print(f\" scraping {url} \")\n",
        "    try:\n",
        "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "        for script_or_style in soup(['script', 'style']):\n",
        "            script_or_style.decompose()\n",
        "        text = '\\n'.join(chunk for chunk in (phrase.strip() for line in (line.strip() for line in soup.get_text().splitlines()) for phrase in line.split(\"  \")) if chunk)\n",
        "        return text if text else \"Could not find any readable text on the page.\"\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        return f\"Error fetching the URL: {e}\"\n",
        "\n",
        "@tool\n",
        "def chart_generator(data_summary: str) -> str:\n",
        "    \"\"\"Generates a Vega-Lite JSON spec from a data summary and saves it as a PNG image.\"\"\"\n",
        "    print(f\" generating and rendering chart from summary: '{data_summary[:30]}...' \")\n",
        "    output_filename = \"chart_visualization.png\"\n",
        "    prompt = f\"\"\"You are an expert data visualization assistant. Convert the following summary into a valid Vega-Lite JSON specification for a bar chart. The data MUST be embedded directly in the 'values' field (not as a named dataset array). Output ONLY the JSON object in a ```json ... ``` block.\n",
        "\n",
        "Data Summary:\n",
        "\\\"{data_summary}\\\"\n",
        "\"\"\"\n",
        "    try:\n",
        "        response = granite_worker_llm.invoke(prompt)\n",
        "        json_spec_str = response.content.split('```json')[1].split('```')[0].strip()\n",
        "        spec_dict = json.loads(json_spec_str)\n",
        "\n",
        "        data_field = spec_dict.get(\"data\")\n",
        "        if isinstance(data_field, list) and data_field:\n",
        "            first_ds = data_field[0]\n",
        "            if \"values\" in first_ds:\n",
        "                spec_dict[\"data\"] = {\"values\": first_ds[\"values\"]}\n",
        "\n",
        "        png_data = vlc.vegalite_to_png(vl_spec=spec_dict, scale=2)\n",
        "        with open(output_filename, \"wb\") as f:\n",
        "            f.write(png_data)\n",
        "\n",
        "        return f\"[Chart successfully generated and saved to file: {output_filename}]\"\n",
        "    except Exception as e:\n",
        "        return f\"Error: Could not generate or save the chart. Details: {e}\"\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "qzfYsz3FTfLy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **Step 4: Define Agent State and Roles**\n",
        "\n",
        "We define the shared `state` for our graph and create the specific agent `roles`. The `agent_map` is crucial for routing tasks to the correct model and tool configuration."
      ],
      "metadata": {
        "id": "IFpTz-pqTfLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from enum import Enum\n",
        "from typing import TypedDict, List, Dict, Annotated, Optional\n",
        "import operator\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "\n",
        "class Role(str, Enum):\n",
        "    SEARCHER        = \"Searcher\"\n",
        "    WEB_SCRAPER     = \"Web Scraper\"\n",
        "    NOTE_TAKER      = \"Note Taker\"\n",
        "    CHART_GENERATOR = \"Chart Generator\"\n",
        "    WRITER          = \"Writer\"\n",
        "\n",
        "class Task(BaseModel):\n",
        "    role: Role = Field(description=\"The role responsible for the task.\")\n",
        "    description: str = Field(description=\"A detailed description of the task.\")\n",
        "\n",
        "class SubTeam(BaseModel):\n",
        "    team: str = Field(\n",
        "        description=\"The name of the sub-team (e.g., 'Research Team', 'Document Authoring').\"\n",
        "    )\n",
        "    tasks: List[Task]\n",
        "\n",
        "# Wrapper model for structured output\n",
        "class SubTeamAssignments(BaseModel):\n",
        "    sub_team_assignments: List[SubTeam]\n",
        "\n",
        "class TeamState(TypedDict):\n",
        "    user_request: str\n",
        "    sub_team_assignments: Optional[List[SubTeam]]\n",
        "    research_results: Annotated[List[Dict], operator.add]\n",
        "    authoring_results: Annotated[List[Dict], operator.add]\n",
        "    final_report: Optional[str]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "5KkhrWFcTfLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "agent_map = {\n",
        "    Role.SEARCHER:        granite_worker_llm.bind_tools([web_search]),\n",
        "    Role.WEB_SCRAPER:     granite_worker_llm.bind_tools([web_scraper]),\n",
        "    Role.NOTE_TAKER:      granite_worker_llm,\n",
        "    Role.CHART_GENERATOR: granite_worker_llm.bind_tools([chart_generator]),\n",
        "    Role.WRITER:          mistral_worker_llm,\n",
        "}"
      ],
      "metadata": {
        "id": "MWQ5MSHmeXT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **Step 5: Define Nodes and Supervisor Logic**\n",
        "\n",
        "We'll define three main nodes: a generic `worker_node`, a `sub_team_node` to manage the parallel execution within each sub-team, and the `Supervisor` node to orchestrate the entire workflow."
      ],
      "metadata": {
        "id": "aj88K4alTfLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "def worker_node(task: Task) -> dict:\n",
        "    agent = agent_map[task.role]\n",
        "    print(f\"[DEBUG] Worker '{task.role.value}' selected agent: {agent}\")\n",
        "    print(f\" 👷 Worker '{task.role.value}' starting task: {task.description} \")\n",
        "    res = agent.invoke([HumanMessage(content=task.description)])\n",
        "    # If the runnable returned tool calls, execute and log them\n",
        "    if hasattr(res, \"tool_calls\") and res.tool_calls:\n",
        "        out = \"\"\n",
        "        for tc in res.tool_calls:\n",
        "            name = tc[\"name\"]\n",
        "            args = tc[\"args\"]\n",
        "            print(f\"[DEBUG] Tool call detected: '{name}' with args {args}\")\n",
        "            tool = {\n",
        "                \"web_search\": web_search,\n",
        "                \"web_scraper\": web_scraper,\n",
        "                \"chart_generator\": chart_generator,\n",
        "            }[name]\n",
        "            print(f\"[DEBUG] Invoking tool '{name}'... \")\n",
        "            result = tool.invoke(args)\n",
        "            print(f\"[DEBUG] Tool '{name}' result: {result}\")\n",
        "            out += str(result)\n",
        "        return {\"role\": task.role, \"result\": out}\n",
        "    # Otherwise, use direct LLM output\n",
        "    print(f\"[DEBUG] No tool calls, using LLM content output.\")\n",
        "    return {\"role\": task.role, \"result\": res.content}\n",
        "\n",
        "def sub_team_node(state: TeamState, team_name: str) -> dict:\n",
        "    teams = [t for t in state[\"sub_team_assignments\"] if t.team == team_name]\n",
        "    if not teams:\n",
        "        return {}\n",
        "    results = [worker_node(task) for task in teams[0].tasks]\n",
        "    return {\"research_results\": results} if team_name == \"Research Team\" else {\"authoring_results\": results}\n",
        "\n",
        "def choose_branch(state):\n",
        "    research_empty = not state[\"research_results\"]\n",
        "    authoring_empty = not state[\"authoring_results\"]\n",
        "    print(f\"[DEBUG] Guard: research_empty={research_empty}, authoring_empty={authoring_empty}\")\n",
        "    if research_empty:\n",
        "        print(\"[DEBUG] Branch -> research_team\")\n",
        "        return \"research\"\n",
        "    if authoring_empty:\n",
        "        print(\"[DEBUG] Branch -> document_authoring_team\")\n",
        "        return \"authoring\"\n",
        "    print(\"[DEBUG] Branch -> end\")\n",
        "    return \"end\"\n",
        "\n",
        "class Supervisor:\n",
        "    def __init__(self, llm):\n",
        "        self.llm = llm\n",
        "        self.structured_llm = llm.with_structured_output(SubTeamAssignments)\n",
        "\n",
        "    def __call__(self, state: TeamState):\n",
        "        if state.get(\"sub_team_assignments\") is None:\n",
        "            print(\" 🧑‍💼 Supervisor: decomposing request \")\n",
        "            prompt = f\"\"\"\n",
        "You are the Supervisor of a multi-disciplinary AI team. Turn this user request into exactly two SubTeam entries:\n",
        "\n",
        "- **Research Team**\n",
        "  - Roles: Searcher, Web Scraper, Note Taker\n",
        "- **Document Authoring Team**\n",
        "  - Roles: Writer, Chart Generator\n",
        "  - MUST include: a Writer task and a Chart Generator task.\n",
        "\n",
        "Use only these five roles. Output only valid JSON for SubTeamAssignments.\n",
        "\n",
        "User Request:\n",
        "{state['user_request']}\n",
        "\"\"\"\n",
        "            response = self.structured_llm.invoke(prompt)\n",
        "            print(f\"[DEBUG] Supervisor decomposed to: {response.sub_team_assignments}\")\n",
        "            return {\"sub_team_assignments\": response.sub_team_assignments}\n",
        "\n",
        "        research_done = bool(state[\"research_results\"])\n",
        "        authoring_done = bool(state[\"authoring_results\"])\n",
        "        if not (research_done and authoring_done):\n",
        "            return {}\n",
        "\n",
        "        print(\" 🧑‍💼 Supervisor: synthesizing final report \")\n",
        "        research_md = \"\\n\".join(f\"### {r['role'].value}\\n{r['result']}\" for r in state[\"research_results\"])\n",
        "        authoring_md = \"\\n\".join(f\"### {r['role'].value}\\n{r['result']}\" for r in state[\"authoring_results\"])\n",
        "        context = f\"## Research Results\\n\\n{research_md}\\n\\n## Document Components\\n\\n{authoring_md}\"\n",
        "        prompt = (\n",
        "            \"You are the Supervisor. Assemble all team findings into a final report \"\n",
        "            f\"that addresses: {state['user_request']}\\n\\n{context}\"\n",
        "        )\n",
        "        final = self.llm.invoke(prompt).content\n",
        "        print(f\"[DEBUG] Final report content generated.\")\n",
        "        return {\"final_report\": final}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "F3UUQWd0TfLz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **Step 6: Construct the Hierarchical Graph**\n",
        "\n",
        "Now we wire the nodes together according to the diagram. The supervisor routes work to the two sub-teams, which run in parallel."
      ],
      "metadata": {
        "id": "NLaRmb3DTfL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.graph import StateGraph, END, START\n",
        "\n",
        "# instantiate the Supervisor\n",
        "supervisor_node = Supervisor(supervisor_llm)\n",
        "\n",
        "workflow = StateGraph(TeamState)\n",
        "workflow.add_node(\"supervisor\", supervisor_node.__call__)\n",
        "workflow.add_node(\"research_team\", lambda s: sub_team_node(s, \"Research Team\"))\n",
        "workflow.add_node(\"document_authoring_team\", lambda s: sub_team_node(s, \"Document Authoring Team\"))\n",
        "workflow.add_edge(START, \"supervisor\")\n",
        "workflow.add_conditional_edges(\n",
        "    \"supervisor\", choose_branch,\n",
        "    {\"research\": \"research_team\", \"authoring\": \"document_authoring_team\", \"end\": END}\n",
        ")\n",
        "workflow.add_edge(\"research_team\", \"supervisor\")\n",
        "workflow.add_edge(\"document_authoring_team\", \"supervisor\")\n",
        "\n",
        "graph = workflow.compile()\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "9nXvowUNTfL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image, display\n",
        "\n",
        "try:\n",
        "    display(Image(graph.get_graph().draw_mermaid_png()))\n",
        "except Exception:\n",
        "    # This requires some extra dependencies and is optional\n",
        "    pass"
      ],
      "metadata": {
        "id": "4EEoaOGnUrEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **Step 7: Run the Team with Full Observability**\n",
        "\n",
        "Finally, let's give our AI team a request and observe their work in Langfuse."
      ],
      "metadata": {
        "id": "k0bzqTAKTfL0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.runnables import RunnableConfig\n",
        "from langfuse.langchain import CallbackHandler\n",
        "from langgraph.errors import GraphRecursionError\n",
        "from langfuse import get_client\n",
        "\n",
        "client            = get_client()\n",
        "langfuse_handler  = CallbackHandler()\n",
        "\n",
        "# Define the configuration for the run, enabling Langfuse tracing\n",
        "config = RunnableConfig(\n",
        "    callbacks=[langfuse_handler],\n",
        "    recursion_limit=10,\n",
        "    configurable={\n",
        "        \"session_id\": \"nested-hierarchy-run-1\",\n",
        "        \"thread_id\":  \"nested-hierarchy-thread-1\"\n",
        "        }\n",
        ")\n",
        "\n",
        "# Define the initial user request\n",
        "initial_state = {\"user_request\": \"Generate a report on the current impact of AI on the US job market as of August 2025, including a chart summarizing key statistics.\"}\n",
        "\n",
        "print(\" Launching the AI Reporting Team...\")\n",
        "print(\"Check your Langfuse project for the live trace!\")\n",
        "\n",
        "try:\n",
        "    final_state = graph.invoke(initial_state, config=config)\n",
        "    client.flush()\n",
        "\n",
        "    #sid = config.configurable[\"session_id\"]\n",
        "    #print(f\"🔗 View your run in Langfuse: https://us.cloud.langfuse.com/app/<YOUR-PROJECT-ID>/sessions/{sid}\")\n",
        "\n",
        "except GraphRecursionError as e:\n",
        "    print(f\"⚠️ Recursion limit reached: {e}\")\n",
        "    client.flush()\n",
        "\n",
        "    #sid = config.configurable[\"session_id\"]\n",
        "    #print(f\"🔗 View partial run in Langfuse: https://us.cloud.langfuse.com/app/<YOUR-PROJECT-ID>/sessions/{sid}\")\n",
        "\n",
        "    # fetch the last persisted state\n",
        "    snapshot = graph.get_state(config)\n",
        "    partial = snapshot.values.get(\"press_release_components\", [])\n",
        "\n",
        "    partial_release = \"\\n\\n\\n\\n\".join(\n",
        "        f\"### Component from {c['role']}:\\n{c['component']}\"\n",
        "        for c in partial\n",
        "    )\n",
        "\n",
        "    print(\"\\n PARTIAL RELEASE:\")\n",
        "    print(partial_release)\n",
        "\n",
        "    final_state = {\"final_report\": partial_release}\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"✅ FINAL REPORT\")\n",
        "print(\"=\"*60)\n",
        "print(final_state[\"final_report\"])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "id": "Mp9ggjIWTfL1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_state[\"authoring_results\"]"
      ],
      "metadata": {
        "id": "hCZJRwb03OPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re, os\n",
        "from IPython.display import Image, display\n",
        "\n",
        "filename = None\n",
        "for comp in final_state.get(\"authoring_results\", []):\n",
        "    if comp[\"role\"] == \"Chart Generator\":\n",
        "        m = re.search(r\"saved to file:\\s*(\\S+\\.png)\", comp[\"result\"])\n",
        "        if m:\n",
        "            filename = m.group(1)\n",
        "            break\n",
        "\n",
        "if not filename:\n",
        "    print(\"⚠️ No chart file mentioned in authoring_results.\")\n",
        "else:\n",
        "    if os.path.exists(filename):\n",
        "        print(f\"✅ Found chart at {filename}, displaying it now:\")\n",
        "        display(Image(filename=filename))\n",
        "    else:\n",
        "        print(f\"❌ Chart mentioned as {filename}, but file not found on disk.\")\n"
      ],
      "metadata": {
        "id": "IzwGkztg3Lsx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}