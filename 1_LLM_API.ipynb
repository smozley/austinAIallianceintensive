{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "D9iDbLa1u-Zv",
        "b69mp6XgvHdJ"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smozley/austinAIallianceintensive/blob/main/1_LLM_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM API Integration"
      ],
      "metadata": {
        "id": "D9iDbLa1u-Zv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this demo, we will send an LLM a prompt template and see what our response looks like"
      ],
      "metadata": {
        "id": "BXq2cAWj9JO_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's use the same prompt template we just used as an example:\n",
        "```\n",
        "Given the following data for {{ticker}}, predict whether the stock will go up, down, or stay stable over the next {{prediction_window}} days. Justify your answer.\n",
        "\n",
        "Earnings: {{earnings_summary}}\n",
        "News: {{news_summary}}\n",
        "History: {{price_trend}}\n",
        "\n",
        "Respond with: [\"Up\", \"Down\", or \"Stable\"] and a brief explanation.\n",
        "```\n",
        "\n",
        "Variables needed:\n",
        "- `ticker`\n",
        "- `prediction_window`\n",
        "- `earnings_summary`\n",
        "- `news_summary`\n",
        "- `price_trend`"
      ],
      "metadata": {
        "id": "jUeE4s6lvS3u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's manually define the first two.\n",
        "ticker = \"TSLA\"\n",
        "prediction_window = 10"
      ],
      "metadata": {
        "id": "AZm3ZSrXIL3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initial Setup"
      ],
      "metadata": {
        "id": "cW8_6Cvr5RKB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Claude Setup"
      ],
      "metadata": {
        "id": "46OLkO6M5dNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's install our dependencies first.\n",
        "!pip install anthropic"
      ],
      "metadata": {
        "id": "NsgBMioIx57t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First let's establish our client. For this we're going to use Anthropic's APIs\n",
        "# Be sure to set your ANTHROPI_API_KEY in the secrets tab.\n",
        "import anthropic\n",
        "from google.colab import userdata\n",
        "\n",
        "client = anthropic.Anthropic(api_key=userdata.get('ANTHROPIC_API_KEY'))\n",
        "\n",
        "# Let's make sure it's working by sending a quick message:\n",
        "message = client.messages.create(\n",
        "    model=\"claude-sonnet-4-20250514\",\n",
        "    max_tokens=1024,\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Hello, Claude\"}\n",
        "    ]\n",
        ")\n",
        "print(message.content[0].text)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "-oL5OSydwhOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's create a more abstract function that will allow us to send a model ID with our prompts.\n",
        "from pydantic import BaseModel\n",
        "from enum import StrEnum\n",
        "\n",
        "class Model(StrEnum):\n",
        "  SM = 'claude-3-5-haiku-20241022'\n",
        "  MD = 'claude-sonnet-4-20250514'\n",
        "  LG = 'claude-opus-4-20250514'\n",
        "\n",
        "class Question(BaseModel):\n",
        "  model: Model\n",
        "  prompt: str\n",
        "\n",
        "\n",
        "def claude(question: Question) -> str:\n",
        "  message = client.messages.create(\n",
        "    model=question.model.value,\n",
        "    max_tokens=1024,\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": question.prompt}\n",
        "    ]\n",
        "  )\n",
        "  return message.content[0].text"
      ],
      "metadata": {
        "id": "hvjhdKDo1MK1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's see what claude knows from the top of it's head\n",
        "question = Question(model=Model.SM, prompt=f\"What's the latest information you have about {ticker}\")\n",
        "\n",
        "response = claude(question)\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "id": "PhwTrES3wgQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stock Market API Setup"
      ],
      "metadata": {
        "id": "vHYlWsrm5kxV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Go to [AlphVantage's Website](https://www.alphavantage.co/) and create a free API key."
      ],
      "metadata": {
        "id": "2hZVQGHh7a2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "AV_API_KEY = userdata.get('AV_API_KEY')"
      ],
      "metadata": {
        "id": "Ej11R42MCWEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Let's test and make sure we get data\n",
        "url = f'https://www.alphavantage.co/query?function=EARNINGS&symbol={ticker}&apikey={AV_API_KEY}'\n",
        "r = requests.get(url)\n",
        "data = r.json()\n",
        "\n",
        "print(data)"
      ],
      "metadata": {
        "id": "9ZXn7KCX5bp5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prompt Templates"
      ],
      "metadata": {
        "id": "MWNjsXtK8_u7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are the variables we still need to get before sending a templated prompt to the LLM\n",
        "\n",
        "- `earnings_summary`\n",
        "- `news_summary`\n",
        "- `price_trend`"
      ],
      "metadata": {
        "id": "mtT4lgXU9WYN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First let's create a function that will get us the recent earnings from AV and then generate\n",
        "# a summary of the recent earnings\n",
        "\n",
        "def get_earnings_summary(ticker: str) -> str:\n",
        "  # Make the request to AV\n",
        "  url = f'https://www.alphavantage.co/query?function=EARNINGS&symbol={ticker}&apikey={AV_API_KEY}'\n",
        "  r = requests.get(url)\n",
        "  data = r.json()\n",
        "\n",
        "  # Now let's ask the LLM to create a short summary of our information\n",
        "  question = Question(model=Model.SM, prompt=f\"Here's the lastest information on {ticker}'s earnings. Today's date is 2025-08-04. Create a short summary of what you see here: {data}\")\n",
        "  response = claude(question)\n",
        "\n",
        "  return response\n",
        "\n",
        "# Let's test\n",
        "earnings_summary = get_earnings_summary(ticker)\n",
        "print(earnings_summary)"
      ],
      "metadata": {
        "id": "a1aUzxRm8OMc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Next let's get the news summary in the same way we got the earnings summary\n",
        "\n",
        "def get_news_summary(ticker: str) -> str:\n",
        "  # Make the request to AV\n",
        "  url = f'https://www.alphavantage.co/query?function=NEWS_SENTIMENT&tickers={ticker}&apikey={AV_API_KEY}'\n",
        "  r = requests.get(url)\n",
        "  data = r.json()\n",
        "\n",
        "  # Summarize\n",
        "  question = Question(model=Model.SM, prompt=f\"Here's the lastest information on {ticker}'s news including the sentiment. Create a short summary of what you see here: {data}\")\n",
        "  response = claude(question)\n",
        "\n",
        "  return response\n",
        "\n",
        "# Let's test\n",
        "news_summary = get_news_summary(ticker)\n",
        "print(news_summary)"
      ],
      "metadata": {
        "id": "tlv-6u_uBzkF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finally let's get the price trend\n",
        "\n",
        "def get_price_trend(ticker: str, days: int) -> dict:\n",
        "  # Make the request to AV\n",
        "  url = f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={ticker}&apikey={AV_API_KEY}'\n",
        "  r = requests.get(url)\n",
        "  data = r.json()\n",
        "  time_series = data.get(\"Time Series (Daily)\", {})\n",
        "  sorted_dates = sorted(time_series.keys(), reverse=True)  # most recent first\n",
        "\n",
        "  # Truncate to the most recent `days` entries\n",
        "  prices = {date: time_series[date] for date in sorted_dates[:days]}\n",
        "\n",
        "  return prices\n",
        "\n",
        "# Let's test\n",
        "price_trend = get_price_trend(ticker, prediction_window)\n",
        "print(price_trend)"
      ],
      "metadata": {
        "id": "1beaUybtDbBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have all the data we need let's get to templating"
      ],
      "metadata": {
        "id": "flgJqgAXF2Mt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = f\"\"\"\n",
        "Given the following data for {ticker}, predict whether the stock will go up, down, or stay stable over the next 2 days, based on the last {prediction_window} days of data. Justify your answer.\n",
        "\n",
        "## Earnings:\n",
        "{earnings_summary}\n",
        "\n",
        "## News:\n",
        "{news_summary}\n",
        "\n",
        "## History:\n",
        "{price_trend}\n",
        "\n",
        "Respond with: **Up**, **Down**, or **Stable** and a brief explanation.\n",
        "\"\"\"\n",
        "\n",
        "question = Question(model=Model.MD, prompt=prompt)\n",
        "response = claude(question)\n",
        "\n",
        "print(response)"
      ],
      "metadata": {
        "id": "tG4tWYYDGK_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streaming vs. Non-Streaming"
      ],
      "metadata": {
        "id": "PdrDzqF8JsEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Generator\n",
        "\n",
        "# Let's create another abstract function to handle streams\n",
        "def stream_claude(question: Question) -> Generator[str, None, None]:\n",
        "  # Only MD and LG support streaming\n",
        "  assert question.model in [Model.MD, Model.LG]\n",
        "\n",
        "  with client.messages.stream(\n",
        "        model=question.model.value,\n",
        "        max_tokens=1024,\n",
        "        messages=[{\"role\": \"user\", \"content\": question.prompt}]\n",
        "    ) as stream:\n",
        "        for text in stream.text_stream:\n",
        "            yield text"
      ],
      "metadata": {
        "id": "8ysRe8msJwdg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in stream_claude(question):\n",
        "    print(chunk, end=\"\", flush=True)"
      ],
      "metadata": {
        "id": "5z6lM0CsKOw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## JSON Output"
      ],
      "metadata": {
        "id": "YtZWuZeGL3NE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install instructor"
      ],
      "metadata": {
        "id": "LI6uShZEM86U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's create another abstract function to handle streams\n",
        "\n",
        "# For this we will use an SDK called instructor that acts as a wrapper around AI clients and does things that they don't natively support like JSON mode.\n",
        "import instructor\n",
        "from typing import Type, TypeVar\n",
        "\n",
        "instructor_client = instructor.from_anthropic(client=client)\n",
        "\n",
        "T = TypeVar(\"T\", bound=BaseModel)\n",
        "\n",
        "def generate_structured_response(question: Question, response_model: Type[T]) -> T:\n",
        "    return instructor_client.chat.completions.create(\n",
        "        model=question.model.value,\n",
        "        max_tokens=1024,\n",
        "        messages=[{\"role\": \"user\", \"content\": question.prompt}],\n",
        "        response_model=response_model,\n",
        "    )"
      ],
      "metadata": {
        "id": "SDnQu9EGL6C8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's define a BaseModel and see how our response comes out.\n",
        "\n",
        "# Our desired model\n",
        "class StockInformation(BaseModel):\n",
        "  ticker: str\n",
        "  technical_analysis: str\n",
        "  fundamental_analysis: str\n",
        "  market_sentiment: str\n",
        "\n",
        "# Our response\n",
        "response = generate_structured_response(question, response_model=StockInformation)\n",
        "print(\"Ticker:\", response.ticker)\n",
        "print(\" \")\n",
        "print(\"Technical Analysis:\", response.technical_analysis)\n",
        "print(\" \")\n",
        "print(\"Fundamental Analysis:\", response.fundamental_analysis)\n",
        "print(\" \")\n",
        "print(\"Market Sentiment:\", response.market_sentiment)"
      ],
      "metadata": {
        "id": "CGCuwrXIOFAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response.model_dump json"
      ],
      "metadata": {
        "id": "XaaBenx_d_be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tool Use"
      ],
      "metadata": {
        "id": "b69mp6XgvHdJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tool & Schema Definition"
      ],
      "metadata": {
        "id": "0lC0nR3PvLhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First, let's define the tool input models, The LLM will use this and know what to input.\n",
        "from pydantic import Field, BaseModel\n",
        "from typing import Any, Callable\n",
        "\n",
        "\n",
        "class GetNewsInput(BaseModel):\n",
        "    ticker: str = Field(..., description=\"Stock ticker symbol, e.g., TSLA\")\n",
        "\n",
        "class GetEarningsInput(BaseModel):\n",
        "    ticker: str = Field(..., description=\"Stock ticker symbol, e.g., TSLA\")\n",
        "\n",
        "class GetPriceTrendInput(BaseModel):\n",
        "    ticker: str = Field(..., description=\"Stock ticker symbol, e.g., TSLA\")\n",
        "    days: int = Field(..., description=\"Number of past trading days to analyze\")\n",
        "\n",
        "# Now let's rewrite the existing functions that will do the same thing except this time, we will not use the LLM to summarize.\n",
        "# These are the callable tool functions (think of these as handlers)\n",
        "def get_news(input: GetNewsInput) -> str:\n",
        "  url = f'https://www.alphavantage.co/query?function=NEWS_SENTIMENT&tickers={input.ticker}&apikey={AV_API_KEY}'\n",
        "  r = requests.get(url)\n",
        "  data = r.json()\n",
        "  return str(data)\n",
        "\n",
        "def get_earnings(input: GetEarningsInput) -> str:\n",
        "  url = f'https://www.alphavantage.co/query?function=EARNINGS&symbol={input.ticker}&apikey={AV_API_KEY}'\n",
        "  r = requests.get(url)\n",
        "  data = r.json()\n",
        "  return str(data)\n",
        "\n",
        "def get_price_trends(input: GetPriceTrendInput) -> str:\n",
        "  url = f'https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={input.ticker}&apikey={AV_API_KEY}'\n",
        "  r = requests.get(url)\n",
        "  data = r.json()\n",
        "  time_series = data.get(\"Time Series (Daily)\", {})\n",
        "  sorted_dates = sorted(time_series.keys(), reverse=True)  # most recent first\n",
        "  prices = {date: time_series[date] for date in sorted_dates[:input.days]}\n",
        "  return str(prices)\n",
        "\n",
        "\n",
        "# Let's create a function that will generate our schma.\n",
        "def to_claude_tool(name: str, description: str, input_model: type[BaseModel]) -> dict:\n",
        "    return {\n",
        "        \"name\": name,\n",
        "        \"description\": description,\n",
        "        \"input_schema\": input_model.model_json_schema()\n",
        "    }\n",
        "\n",
        "# Define tools for Claude this will generate a list of the tool input schema that Claude can read from.\n",
        "claude_tools = [\n",
        "    to_claude_tool(\"get_news\", \"Get the latest news sentiment information for a given stock ticker\", GetNewsInput),\n",
        "    to_claude_tool(\"get_earnings\", \"Get the latest earnings for a given stock ticker\", GetEarningsInput),\n",
        "    to_claude_tool(\"get_price_trends\", \"Get the recent price trend for a stock over N days\", GetPriceTrendInput),\n",
        "]\n",
        "\n",
        "\n",
        "TOOL_FUNCTIONS: dict[str, Callable[[Any], str]] = {\n",
        "    \"get_news\": get_news,\n",
        "    \"get_earnings\": get_earnings,\n",
        "    \"get_price_trends\": get_price_trends,\n",
        "}"
      ],
      "metadata": {
        "id": "MieeojrGvOwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Streaming with Tools"
      ],
      "metadata": {
        "id": "kU0Rlxx0O3Ly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a system prompt that will have a specific instruction to the LLM about generating the stock ticker.\n",
        "system_prompt = \"You are an assistant that aids with stock market research. If you decide to use a tool, you must first determine (from your own knowledge) the ticker of the company that you want to research\""
      ],
      "metadata": {
        "id": "XvxZf0gCOnyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now let's create another abstract function that will create a streaming request but with tools.\n",
        "def stream_claude_with_tools(question: Question) -> Generator[str, None, None]:\n",
        "    \"\"\"Stream Claude response with tool support\"\"\"\n",
        "    assert question.model in [Model.MD, Model.LG]\n",
        "\n",
        "    messages = [{\"role\": \"user\", \"content\": question.prompt}]\n",
        "\n",
        "    while True:\n",
        "        # First, make a non-streaming call to get the complete response\n",
        "        response = client.messages.create(\n",
        "            model=question.model.value,\n",
        "            max_tokens=1024,\n",
        "            tools=claude_tools,\n",
        "            tool_choice={\"type\": \"auto\"},\n",
        "            messages=messages,\n",
        "            system=system_prompt\n",
        "        )\n",
        "\n",
        "        # Stream the text content to the user\n",
        "        has_text = False\n",
        "        for block in response.content:\n",
        "            if block.type == \"text\":\n",
        "                yield block.text\n",
        "                has_text = True\n",
        "\n",
        "        # Extract tool calls\n",
        "        tool_calls = [block for block in response.content if block.type == \"tool_use\"]\n",
        "\n",
        "        # Add assistant response to messages (this must include ALL content blocks)\n",
        "        messages.append({\n",
        "            \"role\": \"assistant\",\n",
        "            \"content\": response.content\n",
        "        })\n",
        "\n",
        "        # If no tools were called, we're done\n",
        "        if not tool_calls:\n",
        "            break\n",
        "\n",
        "        yield \"\\n\\n[Executing tools...]\\n\\n\"\n",
        "\n",
        "        # Execute all tools and collect results\n",
        "        tool_results = []\n",
        "\n",
        "        for tool_call in tool_calls:\n",
        "            tool_name = tool_call.name\n",
        "            tool_input = tool_call.input\n",
        "            tool_id = tool_call.id\n",
        "\n",
        "            yield f\"[Calling {tool_name} with {tool_input}, id: {tool_id}]\\n\"\n",
        "\n",
        "            if tool_name in TOOL_FUNCTIONS:\n",
        "                try:\n",
        "                    # Create the appropriate input object\n",
        "                    if tool_name == \"get_news\":\n",
        "                        input_obj = GetNewsInput(**tool_input)\n",
        "                    elif tool_name == \"get_earnings\":\n",
        "                        input_obj = GetEarningsInput(**tool_input)\n",
        "                    elif tool_name == \"get_price_trends\":\n",
        "                        input_obj = GetPriceTrendInput(**tool_input)\n",
        "                    else:\n",
        "                        raise ValueError(f\"Unknown tool: {tool_name}\")\n",
        "\n",
        "                    # Execute the tool function\n",
        "                    result = TOOL_FUNCTIONS[tool_name](input_obj)\n",
        "                    yield f\"[Tool result received: {len(result)} characters]\\n\"\n",
        "\n",
        "                    # Add to results with exact matching ID\n",
        "                    tool_results.append({\n",
        "                        \"type\": \"tool_result\",\n",
        "                        \"tool_use_id\": tool_id,  # This must match exactly\n",
        "                        \"content\": result\n",
        "                    })\n",
        "\n",
        "                except Exception as e:\n",
        "                    error_msg = f\"Tool execution error: {str(e)}\"\n",
        "                    yield f\"[{error_msg}]\\n\"\n",
        "\n",
        "                    # Add error result with exact matching ID\n",
        "                    tool_results.append({\n",
        "                        \"type\": \"tool_result\",\n",
        "                        \"tool_use_id\": tool_id,  # This must match exactly\n",
        "                        \"content\": error_msg,\n",
        "                        \"is_error\": True\n",
        "                    })\n",
        "            else:\n",
        "                # Tool not found - still need to provide a result\n",
        "                error_msg = f\"Tool '{tool_name}' not implemented\"\n",
        "                yield f\"[{error_msg}]\\n\"\n",
        "                tool_results.append({\n",
        "                    \"type\": \"tool_result\",\n",
        "                    \"tool_use_id\": tool_id,\n",
        "                    \"content\": error_msg,\n",
        "                    \"is_error\": True\n",
        "                })\n",
        "\n",
        "        # Ensure we have a tool result for every tool call\n",
        "        if len(tool_results) != len(tool_calls):\n",
        "            yield f\"[ERROR: Mismatch - {len(tool_calls)} tool calls but {len(tool_results)} results]\\n\"\n",
        "            break\n",
        "\n",
        "        # Add tool results as user message\n",
        "        messages.append({\n",
        "            \"role\": \"user\",\n",
        "            \"content\": tool_results\n",
        "        })\n",
        "\n",
        "        yield \"\\n[Analyzing results...]\\n\\n\""
      ],
      "metadata": {
        "id": "txzNTZomFXBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"What's the last close price for Google?\"\n",
        "\n",
        "question = Question(model=Model.LG, prompt=prompt)\n",
        "\n",
        "for chunk in stream_claude_with_tools(question):\n",
        "    print(chunk, end=\"\", flush=True)"
      ],
      "metadata": {
        "id": "oBD9Vf9GGPPO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}